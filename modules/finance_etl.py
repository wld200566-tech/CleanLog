"""
CleanLog - é‡‘èæ•°æ®æ•´åˆä¸ä¸€è‡´æ€§æ ¡éªŒ
æ¨¡æ‹Ÿé¼ç”²é‡‘èå®¢æˆ·æ•°æ®ä¸€è‡´æ€§æ ¡éªŒåœºæ™¯
"""
import io
import json
from difflib import SequenceMatcher
from pathlib import Path
from typing import Optional, Tuple

import pandas as pd
import streamlit as st

# ä½¿ç”¨å…±äº«ç»„ä»¶ï¼ˆmodules/ui_components.pyï¼Œè‹¥é¡¹ç›®ä¸­æœ‰ utils åˆ™æ”¹ä¸º from utils.ui_components import ...ï¼‰
from utils.ui_components import render_page_header, render_download_section

DINGJIA_SCENARIO = "æ¨¡æ‹Ÿé¼ç”²é‡‘èæ•°æ®æ•´åˆä¸ä¸€è‡´æ€§æ ¡éªŒåœºæ™¯"

# æ ‡å‡† schemaï¼Œç”¨äºè·¨å¹³å°ç»Ÿä¸€
STANDARD_SCHEMA = [
    "timestamp",
    "amount",
    "currency",
    "category",
    "account",
    "counterparty",
    "transaction_id",
    "raw_source",
]

# å„å¹³å° CSV åˆ—å â†’ æ ‡å‡†åˆ— æ˜ å°„æ¨¡æ¿
PLATFORM_SCHEMAS = {
    "alipay": {
        "timestamp": ["åˆ›å»ºæ—¶é—´", "äº¤æ˜“åˆ›å»ºæ—¶é—´", "ä»˜æ¬¾æ—¶é—´"],
        "amount": ["é‡‘é¢"],
        "direction": ["æ”¶/æ”¯", "æ”¶å…¥/æ”¯å‡º"],
        "category": ["ç±»å‹", "äº¤æ˜“ç±»å‹"],
        "counterparty": ["äº¤æ˜“å¯¹æ–¹", "å¯¹æ–¹è´¦æˆ·"],
        "transaction_id": ["è®¢å•å·", "äº¤æ˜“è®¢å•å·"],
    },
    "wechat": {
        "timestamp": ["äº¤æ˜“æ—¶é—´"],
        "amount": ["é‡‘é¢(å…ƒ)", "é‡‘é¢"],
        "direction": ["æ”¶/æ”¯"],
        "category": ["äº¤æ˜“ç±»å‹"],
        "counterparty": ["äº¤æ˜“å¯¹æ–¹", "å•†å“"],
        "transaction_id": ["äº¤æ˜“å•å·", "å•†æˆ·å•å·"],
    },
    "bank": {
        "timestamp": ["äº¤æ˜“æ—¶é—´", "äº¤æ˜“æ—¥æœŸ", "è®°è´¦æ—¶é—´", "äº¤æ˜“æ—¥æœŸæ—¶é—´"],
        "amount": ["é‡‘é¢", "äº¤æ˜“é‡‘é¢", "æ”¶å…¥é‡‘é¢", "æ”¯å‡ºé‡‘é¢"],
        "direction": ["æ”¶ä»˜æ ‡å¿—", "å€Ÿè´·æ ‡å¿—"],
        "category": ["æ‘˜è¦", "äº¤æ˜“æ‘˜è¦", "äº¤æ˜“ç±»å‹"],
        "counterparty": ["å¯¹æ–¹æˆ·å", "äº¤æ˜“å¯¹æ‰‹", "å¯¹æ–¹è´¦å·", "å¯¹æ–¹åç§°"],
        "transaction_id": ["æµæ°´å·", "äº¤æ˜“æµæ°´å·", "å‚è€ƒå·"],
    },
}


def _similarity(a: str, b: str) -> float:
    """è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦ [0, 1]"""
    if pd.isna(a) or pd.isna(b):
        return 0.0
    sa, sb = str(a).strip(), str(b).strip()
    if not sa or not sb:
        return 1.0 if sa == sb else 0.0
    return SequenceMatcher(None, sa, sb).ratio()


def _ensure_columns(df: pd.DataFrame) -> pd.DataFrame:
    """ç¡®ä¿è¾“å‡ºåŒ…å«æ ‡å‡† schema æ‰€æœ‰åˆ—"""
    for col in STANDARD_SCHEMA:
        if col not in df.columns:
            df[col] = pd.NA
    return df[STANDARD_SCHEMA].copy()


class CrossPlatformReconciler:
    """
    è·¨å¹³å°å¯¹è´¦å™¨ - æ¨¡æ‹Ÿé¼ç”²é‡‘èå®¢æˆ·æ•°æ®ä¸€è‡´æ€§æ ¡éªŒ
    æ”¯æŒæ”¯ä»˜å®/å¾®ä¿¡/é“¶è¡Œ CSV è‡ªé€‚åº”è¯†åˆ«ã€æ¨¡ç³ŠåŒ¹é…ã€å•è¾¹è´¦æ£€æµ‹
    """

    def __init__(
        self,
        amount_tolerance: float = 0.01,
        time_window_minutes: int = 5,
        name_similarity_threshold: float = 0.6,
    ):
        self.amount_tolerance = amount_tolerance
        self.time_window_minutes = time_window_minutes
        self.name_similarity_threshold = name_similarity_threshold

    def _detect_platform(self, df: pd.DataFrame, filename: str = "") -> str:
        """æ ¹æ®åˆ—åå’Œæ–‡ä»¶åè¯†åˆ«æ•°æ®æº"""
        cols = set(df.columns)
        fname = filename.lower()

        if "æ”¯ä»˜å®" in filename or "alipay" in fname:
            return "alipay"
        if "å¾®ä¿¡" in filename or "wechat" in fname or "wx" in fname:
            return "wechat"
        if any(k in fname for k in ["bank", "é“¶è¡Œ", "bankcard", "æµæ°´"]):
            return "bank"

        # åˆ—ååŒ¹é…
        for platform, schema in PLATFORM_SCHEMAS.items():
            ts_cands = schema.get("timestamp", [])
            amt_cands = schema.get("amount", []) + schema.get("direction", [])
            if any(c in cols for c in ts_cands) and any(c in cols for c in amt_cands):
                return platform

        return "bank"  # é»˜è®¤å°è¯•é“¶è¡Œæ ¼å¼

    def _build_mapping(self, platform: str, df: pd.DataFrame) -> dict:
        """æ„å»ºåŸå§‹åˆ— â†’ æ ‡å‡†åˆ—æ˜ å°„"""
        schema = PLATFORM_SCHEMAS.get(platform, PLATFORM_SCHEMAS["bank"])
        mapping = {}

        for std_col, cands in schema.items():
            if std_col == "direction":
                continue
            for c in cands:
                if c in df.columns:
                    mapping[c] = std_col
                    break

        return mapping

    def _parse_amount(self, series: pd.Series, direction_series: Optional[pd.Series] = None) -> pd.Series:
        """è§£æé‡‘é¢ï¼šå»é€—å·ã€ç¬¦å·ï¼Œæ ¹æ®æ”¶/æ”¯è®¾æ­£è´Ÿ"""
        s = series.astype(str).str.replace(",", "", regex=False).str.replace("Â¥", "", regex=False)
        vals = pd.to_numeric(s, errors="coerce").fillna(0)

        if direction_series is not None:
            direction = direction_series.astype(str)
            vals = vals.where(
                direction.str.contains("æ”¶å…¥|æ”¶åˆ°|æ”¶æ¬¾|è´·|æ”¶", na=False, regex=True),
                -vals.abs(),
            )
        return vals

    def extract_with_auto_schema(
        self, file_path_or_bytes, filename: str = ""
    ) -> Tuple[pd.DataFrame, str, dict]:
        """
        è‡ªé€‚åº”æå–ï¼šè‡ªåŠ¨è¯†åˆ«å¹³å°å¹¶æ˜ å°„ schema
        æ”¯æŒ CSV ä¸ Excel (.xlsx, .xls)
        è¿”å› (DataFrame, platform, column_mapping)
        """
        def _get_readable():
            """è¿”å›æ¯æ¬¡å¯é‡æ–°è¯»å–çš„æº"""
            if isinstance(file_path_or_bytes, bytes):
                return io.BytesIO(file_path_or_bytes)
            if hasattr(file_path_or_bytes, "getvalue"):
                return io.BytesIO(file_path_or_bytes.getvalue())
            return file_path_or_bytes

        fn_lower = (filename or "").lower()
        is_excel = fn_lower.endswith(".xlsx") or fn_lower.endswith(".xls")

        if is_excel:
            src = _get_readable()
            engine = "openpyxl" if fn_lower.endswith(".xlsx") else None
            df = pd.read_excel(src, engine=engine)
        else:
            for enc in ("utf-8", "gbk", "gb18030"):
                try:
                    src = _get_readable()
                    df = pd.read_csv(src, encoding=enc)
                    break
                except (UnicodeDecodeError, Exception):
                    continue
            else:
                raise ValueError("æ— æ³•è§£ç æ–‡ä»¶ç¼–ç ")

        if df.empty:
            return _ensure_columns(df), "unknown", {}

        platform = self._detect_platform(df, filename)
        mapping = self._build_mapping(platform, df)

        rename_map = {k: v for k, v in mapping.items() if k in df.columns}
        df = df.rename(columns=rename_map)

        # å¹³å°æ ‡è¯†
        platform_labels = {"alipay": "Alipay", "wechat": "WeChat", "bank": "Bank"}
        df["account"] = platform_labels.get(platform, platform)
        df["raw_source"] = platform
        df["currency"] = "CNY"

        # æ—¶é—´è§£æ
        if "timestamp" not in df.columns:
            raise ValueError(f"æœªæ‰¾åˆ°æ—¶é—´åˆ—ï¼Œå½“å‰åˆ—: {list(df.columns)}")
        df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce")
        df = df.dropna(subset=["timestamp"])

        # é‡‘é¢è§£æ
        dir_col = next(
            (c for c in PLATFORM_SCHEMAS.get(platform, {}).get("direction", []) if c in df.columns),
            None,
        )
        if "amount" in df.columns:
            dir_series = df[dir_col] if dir_col else None
            df["amount"] = self._parse_amount(df["amount"], dir_series)
        elif platform == "bank":
            # éƒ¨åˆ†é“¶è¡Œåˆ†å¼€ æ”¶å…¥/æ”¯å‡º åˆ—
            inc_col = next((c for c in ["æ”¶å…¥é‡‘é¢", "è´·æ–¹é‡‘é¢"] if c in df.columns), None)
            exp_col = next((c for c in ["æ”¯å‡ºé‡‘é¢", "å€Ÿæ–¹é‡‘é¢"] if c in df.columns), None)
            if inc_col and exp_col:
                inc = pd.to_numeric(df[inc_col].astype(str).str.replace(",", "", regex=False), errors="coerce").fillna(0)
                exp = pd.to_numeric(df[exp_col].astype(str).str.replace(",", "", regex=False), errors="coerce").fillna(0)
                df["amount"] = inc - exp
            elif inc_col:
                df["amount"] = pd.to_numeric(df[inc_col].astype(str).str.replace(",", "", regex=False), errors="coerce").fillna(0)
            elif exp_col:
                df["amount"] = -pd.to_numeric(df[exp_col].astype(str).str.replace(",", "", regex=False), errors="coerce").fillna(0)
            else:
                df["amount"] = 0.0

        # åˆ é™¤ä¸´æ—¶åˆ—
        df = df.drop(columns=[c for c in df.columns if c.startswith("_")], errors="ignore")

        return _ensure_columns(df), platform, {v: k for k, v in rename_map.items()}

    def fuzzy_match(
        self, df: pd.DataFrame
    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """
        æ¨¡ç³ŠåŒ¹é…ç®—æ³•ï¼šé‡‘é¢ç›¸åŒ + æ—¶é—´Â±5åˆ†é’Ÿ + å¯¹æ–¹åç§°ç›¸ä¼¼åº¦
        è¿”å› (matched, suspected_duplicate, unilateral)
        """
        if df.empty or len(df) < 2:
            unilateral = df.copy()
            unilateral["match_status"] = "unilateral"
            return (
                pd.DataFrame(columns=df.columns.tolist() + ["match_status"]),
                pd.DataFrame(columns=df.columns.tolist() + ["match_status"]),
                unilateral,
            )

        df = df.copy()
        df["_idx"] = range(len(df))

        matched_indices = set()
        suspected_indices = set()

        for i in range(len(df)):
            if i in matched_indices or i in suspected_indices:
                continue
            row_i = df.iloc[i]
            ts_i = row_i["timestamp"]
            amt_i = row_i["amount"]
            name_i = str(row_i.get("counterparty", "") or "")

            for j in range(i + 1, len(df)):
                if j in matched_indices or j in suspected_indices:
                    continue
                row_j = df.iloc[j]
                ts_j = row_j["timestamp"]
                amt_j = row_j["amount"]
                name_j = str(row_j.get("counterparty", "") or "")

                # é‡‘é¢å®¹å·®
                if abs(amt_i - amt_j) > self.amount_tolerance:
                    continue
                # æ—¶é—´çª—å£ Â±5 åˆ†é’Ÿ
                if abs((ts_i - ts_j).total_seconds()) > self.time_window_minutes * 60:
                    continue
                # åç§°ç›¸ä¼¼åº¦
                sim = _similarity(name_i, name_j)
                if sim < self.name_similarity_threshold:
                    continue

                if sim >= 0.9:
                    matched_indices.add(i)
                    matched_indices.add(j)
                else:
                    suspected_indices.add(i)
                    suspected_indices.add(j)
                break  # æ¯ä¸ª i åªåŒ¹é…ä¸€ä¸ª j

        def tag_df(indices: set, status: str) -> pd.DataFrame:
            sub = df[df["_idx"].isin(indices)].copy()
            sub["match_status"] = status
            sub = sub.drop(columns=["_idx"], errors="ignore")
            return sub

        matched = tag_df(matched_indices, "matched")
        suspected = tag_df(suspected_indices, "suspected_duplicate")
        unilateral_indices = set(df["_idx"]) - matched_indices - suspected_indices
        unilateral = tag_df(unilateral_indices, "unilateral")

        return matched, suspected, unilateral

    def consistency_check(
        self, df: pd.DataFrame
    ) -> dict:
        """
        æ•°æ®ä¸€è‡´æ€§æ ¡éªŒï¼šé‡‘é¢å¹³è¡¡æ£€æŸ¥ã€å•è¾¹è´¦æ£€æµ‹
        """
        result = {
            "amount_balanced": True,
            "total_amount": 0.0,
            "unilateral_count": 0,
            "suspected_duplicate_count": 0,
            "matched_count": 0,
            "warnings": [],
        }

        if df.empty:
            return result

        result["total_amount"] = float(df["amount"].sum())

        if "match_status" not in df.columns:
            matched, suspected, unilateral = self.fuzzy_match(df)
            result["matched_count"] = len(matched)
            result["suspected_duplicate_count"] = len(suspected)
            result["unilateral_count"] = len(unilateral)
        else:
            result["matched_count"] = int((df["match_status"] == "matched").sum())
            result["suspected_duplicate_count"] = int((df["match_status"] == "suspected_duplicate").sum())
            result["unilateral_count"] = int((df["match_status"] == "unilateral").sum())

        if abs(result["total_amount"]) > 100:
            result["amount_balanced"] = False
            result["warnings"].append(f"é‡‘é¢ä¸å¹³è¡¡: å‡€é¢ Â¥{result['total_amount']:,.2f}ï¼Œè¯·æ ¸å¯¹å•è¾¹è´¦")

        if result["unilateral_count"] > len(df) * 0.5:
            result["warnings"].append("å•è¾¹è´¦å æ¯”è¿‡é«˜ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®å®Œæ•´æ€§")

        return result

    def build_excel_report(
        self,
        matched: pd.DataFrame,
        suspected: pd.DataFrame,
        unilateral: pd.DataFrame,
        consistency: dict,
    ) -> bytes:
        """ç”Ÿæˆå«å…¬å¼çš„ Excel å·®å¼‚æŠ¥å‘Š"""
        try:
            import openpyxl
            from openpyxl.utils.dataframe import dataframe_to_rows
            from openpyxl.styles import Font, PatternFill
        except ImportError:
            # æ—  openpyxl æ—¶é€€åŒ–ä¸ºåŸºç¡€ Excel
            for engine in ("xlsxwriter", "openpyxl"):
                try:
                    buf = io.BytesIO()
                    with pd.ExcelWriter(buf, engine=engine) as w:
                        matched.to_excel(w, sheet_name="åŒ¹é…æˆåŠŸ", index=False)
                        suspected.to_excel(w, sheet_name="ç–‘ä¼¼é‡å¤", index=False)
                        unilateral.to_excel(w, sheet_name="å•è¾¹è´¦", index=False)
                    return buf.getvalue()
                except ImportError:
                    continue
            return b""

        wb = openpyxl.Workbook()
        header_fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
        header_font = Font(bold=True, color="FFFFFF")

        for name, data in [
            ("åŒ¹é…æˆåŠŸ", matched),
            ("ç–‘ä¼¼é‡å¤", suspected),
            ("å•è¾¹è´¦", unilateral),
        ]:
            if name in wb.sheetnames:
                ws = wb[name]
            else:
                ws = wb.create_sheet(name)
            for r_idx, row in enumerate(dataframe_to_rows(data, index=False, header=True), 1):
                for c_idx, val in enumerate(row, 1):
                    cell = ws.cell(row=r_idx, column=c_idx, value=val)
                    if r_idx == 1:
                        cell.fill = header_fill
                        cell.font = header_font

        # æ±‡æ€» sheetï¼ˆå«å…¬å¼ï¼‰
        ws_sum = wb.create_sheet("æ±‡æ€»", 0)
        ws_sum["A1"] = "å¯¹è´¦æ±‡æ€»"
        ws_sum["A1"].font = Font(bold=True, size=14)
        ws_sum["A2"] = "åŒ¹é…æˆåŠŸç¬”æ•°"
        ws_sum["B2"] = len(matched)
        ws_sum["A3"] = "ç–‘ä¼¼é‡å¤ç¬”æ•°"
        ws_sum["B3"] = len(suspected)
        ws_sum["A4"] = "å•è¾¹è´¦ç¬”æ•°"
        ws_sum["B4"] = len(unilateral)
        ws_sum["A5"] = "æ€»ç¬”æ•°ï¼ˆå…¬å¼ï¼‰"
        ws_sum["B5"] = "=B2+B3+B4"
        ws_sum["A6"] = "é‡‘é¢æ˜¯å¦å¹³è¡¡"
        ws_sum["B6"] = "æ˜¯" if consistency.get("amount_balanced", True) else "å¦"
        ws_sum["A7"] = "å‡€é¢"
        ws_sum["B7"] = consistency.get("total_amount", 0)

        buf = io.BytesIO()
        wb.save(buf)
        return buf.getvalue()


def run_finance_etl(file_paths: list) -> dict:
    """å¯¹å¤–æ¥å£ï¼šåŸºäºè·¯å¾„æ‰§è¡Œè´¢åŠ¡ ETLï¼ˆå…¼å®¹ app.py åŸæœ‰è°ƒç”¨ï¼‰"""
    reconciler = CrossPlatformReconciler()
    all_dfs = []
    all_mappings = []

    for path in file_paths:
        p = Path(path)
        if not p.exists():
            continue
        try:
            df, platform, mapping = reconciler.extract_with_auto_schema(str(p), p.name)
            if not df.empty:
                all_dfs.append(df)
                all_mappings.append({"file": p.name, "platform": platform, "mapping": mapping})
        except Exception:
            pass

    if not all_dfs:
        return {
            "module": "finance_etl",
            "dingjia_scenario": DINGJIA_SCENARIO,
            "problem_discovery": {"æç¤º": "æœªæˆåŠŸåŠ è½½ä»»ä½•è´¦å•æ–‡ä»¶"},
            "cleaning_actions": [],
            "effect_verification": {},
            "details": {},
            "raw_df": pd.DataFrame(),
            "report_json": json.dumps({"error": "no_data"}, ensure_ascii=False),
            "use_streamlit_ui": True,
        }

    merged = pd.concat(all_dfs, ignore_index=True).drop_duplicates()
    matched, suspected, unilateral = reconciler.fuzzy_match(merged)
    consistency = reconciler.consistency_check(
        pd.concat([matched, suspected, unilateral], ignore_index=True)
    )

    excel_bytes = reconciler.build_excel_report(matched, suspected, unilateral, consistency)

    return {
        "module": "finance_etl",
        "dingjia_scenario": DINGJIA_SCENARIO,
        "problem_discovery": {
            "æ€»äº¤æ˜“æ•°": f"{len(merged):,}",
            "åŒ¹é…æˆåŠŸ": f"{len(matched)}",
            "ç–‘ä¼¼é‡å¤": f"{len(suspected)}",
            "å•è¾¹è´¦": f"{len(unilateral)}",
        },
        "cleaning_actions": ["å·²æ‰§è¡Œï¼šSchema è‡ªåŠ¨æ˜ å°„ã€æ¨¡ç³ŠåŒ¹é…ã€ä¸€è‡´æ€§æ ¡éªŒ"],
        "effect_verification": {
            "é‡‘é¢å¹³è¡¡": "æ˜¯" if consistency["amount_balanced"] else "å¦",
            "å‡€é¢ (Â¥)": f"{consistency['total_amount']:,.2f}",
        },
        "details": {
            "consistency": consistency,
            "mappings": all_mappings,
        },
        "raw_df": merged,
        "matched_df": matched,
        "suspected_df": suspected,
        "unilateral_df": unilateral,
        "column_mappings": all_mappings,
        "excel_report_bytes": excel_bytes,
        "report_json": json.dumps(
            {
                "matched": len(matched),
                "suspected_duplicate": len(suspected),
                "unilateral": len(unilateral),
                "consistency": consistency,
            },
            ensure_ascii=False,
            indent=2,
        ),
        "use_streamlit_ui": True,
    }


def render_finance_etl_ui():
    """Streamlit ç•Œé¢ï¼šå¤šæ–‡ä»¶ä¸Šä¼ ã€æ˜ å°„å±•ç¤ºã€ä¸‰æ å¯¹è´¦ç»“æœã€Excel ä¸‹è½½"""
    render_page_header("è´¢åŠ¡å¯¹è´¦", DINGJIA_SCENARIO)

    uploaded_files = st.file_uploader(
        "ä¸Šä¼ è´¦å•æ–‡ä»¶ï¼ˆæ”¯æŒæ”¯ä»˜å®/å¾®ä¿¡/é“¶è¡Œ CSVã€Excelï¼Œå¯å¤šé€‰ï¼‰",
        type=["csv", "xlsx", "xls"],
        accept_multiple_files=True,
        help="åŒæ—¶ä¸Šä¼ å¤šä¸ªå¹³å°è´¦å•è¿›è¡Œè·¨å¹³å°å¯¹è´¦",
    )

    if not uploaded_files:
        st.info("ğŸ‘† è¯·ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ª CSV æˆ– Excel è´¦å•æ–‡ä»¶")
        return

    if st.button("â–¶ å¼€å§‹å¯¹è´¦", type="primary"):
        reconciler = CrossPlatformReconciler()
        all_dfs = []
        all_mappings = []

        with st.spinner("æ­£åœ¨åŠ è½½å¹¶æ˜ å°„æ•°æ®..."):
            for f in uploaded_files:
                try:
                    df, platform, mapping = reconciler.extract_with_auto_schema(
                        f.getvalue(), f.name
                    )
                    if not df.empty:
                        all_dfs.append(df)
                        all_mappings.append({
                            "file": f.name,
                            "platform": platform,
                            "mapping": mapping,
                        })
                except Exception as e:
                    st.warning(f"è·³è¿‡ {f.name}: {e}")

        if not all_dfs:
            st.error("æœªæˆåŠŸåŠ è½½ä»»ä½•è´¦å•")
            return

        merged = pd.concat(all_dfs, ignore_index=True).drop_duplicates()
        matched, suspected, unilateral = reconciler.fuzzy_match(merged)
        consistency = reconciler.consistency_check(
            pd.concat([matched, suspected, unilateral], ignore_index=True)
        )
        excel_bytes = reconciler.build_excel_report(
            matched, suspected, unilateral, consistency
        )

        # è‡ªåŠ¨å­—æ®µæ˜ å°„å±•ç¤ºï¼ˆè¡¨æ ¼ï¼‰
        st.subheader("ğŸ“‹ è‡ªåŠ¨å­—æ®µæ˜ å°„")
        mapping_rows = []
        for m in all_mappings:
            for std_col, orig_col in m["mapping"].items():
                mapping_rows.append({
                    "æ–‡ä»¶": m["file"],
                    "å¹³å°": m["platform"],
                    "æ ‡å‡†åˆ—": std_col,
                    "åŸå§‹åˆ—": orig_col,
                })
        if mapping_rows:
            st.dataframe(pd.DataFrame(mapping_rows), use_container_width=True, hide_index=True)
        else:
            st.caption("æ— é¢å¤–æ˜ å°„ï¼ˆå·²ä½¿ç”¨é»˜è®¤åˆ—åï¼‰")

        # å¯¹è´¦ç»“æœä¸‰æ 
        st.subheader("ğŸ“Š å¯¹è´¦ç»“æœ")
        col1, col2, col3 = st.columns(3)

        with col1:
            st.metric("âœ… åŒ¹é…æˆåŠŸ", len(matched))
            if not matched.empty:
                with st.expander("æŸ¥çœ‹æ•°æ®", expanded=False):
                    st.dataframe(matched, use_container_width=True, hide_index=True)

        with col2:
            st.metric("âš ï¸ ç–‘ä¼¼é‡å¤", len(suspected))
            if not suspected.empty:
                with st.expander("æŸ¥çœ‹æ•°æ®", expanded=False):
                    st.dataframe(suspected, use_container_width=True, hide_index=True)

        with col3:
            st.metric("âŒ å•è¾¹è´¦", len(unilateral))
            if not unilateral.empty:
                with st.expander("æŸ¥çœ‹æ•°æ®", expanded=False):
                    st.dataframe(unilateral, use_container_width=True, hide_index=True)

        # ä¸€è‡´æ€§æ ¡éªŒæ‘˜è¦
        if consistency.get("warnings"):
            st.warning("; ".join(consistency["warnings"]))

        # å·®å¼‚æŠ¥å‘Šä¸‹è½½ï¼ˆExcelï¼‰
        st.subheader("ğŸ“¥ ä¸‹è½½å·®å¼‚æŠ¥å‘Š")
        st.download_button(
            "ä¸‹è½½ Excel æŠ¥å‘Šï¼ˆå«æ±‡æ€»ä¸å…¬å¼ï¼‰",
            excel_bytes,
            file_name="reconciliation_report.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True,
        )

        # ä½¿ç”¨å…±äº«ç»„ä»¶æ¸²æŸ“é¢å¤–ä¸‹è½½ï¼ˆJSON/CSVï¼‰
        render_download_section(
            {
                "report_json": json.dumps(
                    {
                        "matched": len(matched),
                        "suspected_duplicate": len(suspected),
                        "unilateral": len(unilateral),
                        "consistency": consistency,
                    },
                    ensure_ascii=False,
                    indent=2,
                ),
                "raw_df": merged,
            },
            "finance_reconciliation",
        )
