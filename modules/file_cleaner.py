"""
CleanLog - æ–‡ä»¶å»é‡æ¨¡å—
å¯¹åº”é¼ç”²ï¼šä¼ä¸šçº§æ•°æ®æ²»ç†ã€å¤‡ä»½ç´¢å¼•ã€å­˜å‚¨ä¼˜åŒ–
"""
import os
import hashlib
import json
import pandas as pd
from pathlib import Path


DINGJIA_SCENARIO = "ä¼ä¸šçº§æ•°æ®æ²»ç† Â· å¤‡ä»½ç´¢å¼•åº“ Â· å®¢æˆ·ç«¯ Agent æ‰«æ Â· å…¨å±€å»é‡ Â· å­˜å‚¨ä¼˜åŒ–æŠ¥å‘Š"


class FileCleaner:
    """æ–‡ä»¶å»é‡åˆ†æå™¨"""

    def __init__(self, root_paths: list):
        self.root_paths = [Path(p) for p in root_paths]

    def scan_and_index(self):
        """å…¨ç›˜æ‰«æå»ºç«‹ç´¢å¼•"""
        file_index = []
        for root in self.root_paths:
            if not root.exists():
                continue
            for file_path in root.rglob("*"):
                if file_path.is_file():
                    try:
                        stat = file_path.stat()
                        file_index.append({
                            "path": str(file_path),
                            "size": stat.st_size,
                            "mtime": stat.st_mtime,
                            "type": file_path.suffix.lower(),
                            "hash": None,
                        })
                    except (PermissionError, OSError):
                        pass
        return pd.DataFrame(file_index)

    def calculate_hashes(self, df: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—æ–‡ä»¶æŒ‡çº¹"""

        def file_hash(filepath):
            h = hashlib.md5()
            try:
                with open(filepath, "rb") as f:
                    for chunk in iter(lambda: f.read(8192), b""):
                        h.update(chunk)
                return h.hexdigest()
            except (PermissionError, OSError):
                return None

        df = df.copy()
        df["hash"] = df["path"].apply(file_hash)
        return df

    def run(self, df: pd.DataFrame) -> dict:
        """æ‰§è¡Œåˆ†æï¼Œè¿”å›æ ‡å‡†åŒ–æŠ¥å‘Š"""
        if df.empty:
            return self._empty_report()

        df_valid = df[df["hash"].notna()].copy()
        total_files = len(df)
        total_size = df["size"].sum()

        duplicates = df_valid[df_valid.duplicated(subset=["hash"], keep=False)]
        dup_groups = (
            duplicates.groupby("hash")
            .agg({"path": list, "size": "first"})
            .reset_index()
        )

        if dup_groups.empty:
            savings = 0.0
            recommendation = []
        else:
            savings = ((dup_groups["path"].apply(len) - 1) * dup_groups["size"]).sum()
            recommendation = self._generate_recommendation(dup_groups)

        savings_ratio = (savings / total_size * 100) if total_size > 0 else 0.0

        # æ ‡å‡†åŒ–æŠ¥å‘Š
        report = {
            "module": "file_cleaner",
            "dingjia_scenario": DINGJIA_SCENARIO,
            "problem_discovery": {
                "æ‰«ææ–‡ä»¶æ€»æ•°": f"{total_files:,}",
                "æ€»å ç”¨å®¹é‡": f"{total_size / 1e9:.2f} GB",
                "å‘ç°é‡å¤ç»„": f"{len(dup_groups)} ç»„",
                "é‡å¤æ–‡ä»¶æ•°": f"{len(duplicates)} ä¸ª",
            },
            "cleaning_actions": [
                f"å»ºè®®ä¿ç•™æ¯ç»„ä¸­æœ€æ–°çš„æ–‡ä»¶ï¼Œåˆ é™¤å…¶ä½™å‰¯æœ¬",
                f"Top é‡å¤ç»„å¯èŠ‚çœçº¦ {savings / 1e9:.2f} GB",
            ]
            + [f"ä¿ç•™: {r['keep']}ï¼Œå¯åˆ  {len(r['delete'])} ä¸ªå‰¯æœ¬" for r in recommendation[:3]],
            "effect_verification": {
                "å¯èŠ‚çœç©ºé—´ (GB)": f"{savings / 1e9:.2f}",
                "èŠ‚çœæ¯”ä¾‹ (%)": f"{savings_ratio:.1f}%",
                "å»é‡åé¢„ä¼°æ–‡ä»¶æ•°": f"{total_files - len(duplicates) + len(dup_groups):,}",
            },
            "details": {
                "duplicate_groups": len(dup_groups),
                "potential_savings_gb": round(savings / 1e9, 2),
                "recommendation_preview": recommendation[:5],
            },
            "raw_df": df[["path", "size", "type", "hash"]],
            "report_json": json.dumps(
                {
                    "total_files": total_files,
                    "total_size_gb": round(total_size / 1e9, 2),
                    "duplicate_groups": len(dup_groups),
                    "potential_savings_gb": round(savings / 1e9, 2),
                    "savings_ratio": round(savings_ratio, 2),
                },
                ensure_ascii=False,
                indent=2,
            ),
        }
        return report

    def _empty_report(self):
        return {
            "module": "file_cleaner",
            "dingjia_scenario": DINGJIA_SCENARIO,
            "problem_discovery": {"æç¤º": "æœªå‘ç°ä»»ä½•æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„"},
            "cleaning_actions": [],
            "effect_verification": {},
            "details": {},
            "raw_df": pd.DataFrame(),
            "report_json": json.dumps({"error": "no_files"}, ensure_ascii=False),
        }

    def _generate_recommendation(self, dup_groups: pd.DataFrame) -> list:
        actions = []
        for _, row in dup_groups.head(10).iterrows():
            paths = row["path"]
            if not paths:
                continue
            try:
                keep = max(paths, key=lambda p: os.path.getmtime(p))
            except OSError:
                keep = paths[0]
            delete = [p for p in paths if p != keep]
            if delete:
                actions.append({"keep": keep, "delete": delete, "save_space_mb": row["size"] * len(delete) / 1e6})
        return actions


# å…¼å®¹ä¸»å…¥å£å‘½å
FileDeduplicationEngine = FileCleaner


def run_file_cleaner(root_paths: list) -> dict:
    """å¯¹å¤–æ¥å£ï¼šæ‰§è¡Œæ–‡ä»¶å»é‡åˆ†æ"""
    cleaner = FileCleaner(root_paths)
    df = cleaner.scan_and_index()
    df = cleaner.calculate_hashes(df)
    return cleaner.run(df)


def render_file_cleaner_ui():
    """Streamlit ç•Œé¢ï¼šæ–‡ä»¶å»é‡å¼•æ“"""
    import streamlit as st
    from utils.ui_components import (
        render_page_header,
        render_standard_report,
        render_download_section,
        paths_input,
    )

    paths = paths_input(
        "æ‰«æç›®å½•ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰",
        [str(Path.home() / "Downloads"), str(Path.home() / "Documents")],
        help_text="è¾“å…¥è¦æ‰«æçš„æ–‡ä»¶å¤¹è·¯å¾„ï¼Œæ¯è¡Œä¸€ä¸ª",
    )
    st.caption("ğŸ’¡ å»ºè®®ï¼šå…ˆé€‰æ‹©å°ç›®å½•æµ‹è¯•ï¼Œé¿å…é¦–æ¬¡æ‰«æè¿‡ä¹…")

    if st.button("â–¶ å¼€å§‹æ‰«æåˆ†æ", type="primary"):
        with st.spinner("æ­£åœ¨æ‰«ææ–‡ä»¶å¹¶è®¡ç®—æŒ‡çº¹..."):
            try:
                report = run_file_cleaner(paths)
                render_page_header("æ–‡ä»¶å»é‡", report["dingjia_scenario"])
                render_standard_report(report)
                render_download_section(report, "file_cleaner")
            except Exception as e:
                st.error(f"æ‰§è¡Œå¤±è´¥: {e}")
    else:
        render_page_header("æ–‡ä»¶å»é‡", DINGJIA_SCENARIO)
        st.info("ğŸ‘† åœ¨ä¾§è¾¹æ è¾“å…¥æ‰«æç›®å½•åï¼Œç‚¹å‡»è¿è¡ŒæŒ‰é’®")
